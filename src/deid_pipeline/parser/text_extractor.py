import os
import json
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
from docx import Document
import fitz  # PyMuPDF
from .ocr import get_ocr_reader
from ..config import OCR_THRESHOLD, USE_STUB

# only text will be extracted!
# åœ–ç‰‡è£¡é¢çš„æ–‡å­—ç„¡æ³•æå–
def extract_text(file_path: str, ocr: bool=False) -> str:
    ext = os.path.splitext(file_path)[-1].lower()

    if ext == ".txt":
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

    elif ext == ".docx":
        doc = Document(file_path)
        return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

    # pdf å¦‚æœå¤ªéæ–¼è¤‡é›œ(æ ¼å­éå¤šã€ç·¨æ’å¤šæ¨£)ï¼Œå¯èƒ½è®€å–åˆ°çš„è³‡æ–™å°±æœƒæ²’æœ‰æŒ‰ç…§é‚è¼¯
    # é€™ä¸€è¡Œæ˜¯ã€Œèº«åˆ†è­‰å­—è™Ÿã€ï¼Œä¸‹ä¸€è¡Œä¸æœƒæ˜¯ã€Œæ•¸å­—ã€ï¼Œå¯èƒ½æ˜¯åˆ¥çš„æ¬„ä½æˆ–åˆ¥çš„æ–‡å­—å…§å®¹
    elif ext == ".pdf":
        try:
            pdf = fitz.open(file_path)
        except fitz.FileNotFoundError:
            raise RuntimeError(f"File not found: {file_path}")
        except fitz.FileDataError:
            raise RuntimeError(f"Corrupted PDF: {file_path}")

        full_text = []
        for page in pdf:
            # 1) å…ˆåšã€Œå€å¡Šæ’åºã€æå–
            blocks = page.get_text("blocks", sort=True)
            page_text = "\n".join(b[4] for b in blocks if b[4].strip())

            # 2) å°‘é‡æ–‡å­—æ‰è§¸ç™¼ OCR
            if ocr and not USE_STUB and len(page_text) < OCR_THRESHOLD:
                reader = get_ocr_reader()
                img = page.get_pixmap().samples
                h, w = int(page.rect.height), int(page.rect.width)
                arr = np.frombuffer(img, dtype=np.uint8).reshape((h, w, 3))
                ocr_lines = [t[1] for t in reader.readtext(arr)]
                full_text.append("\n".join(ocr_lines))
            else:
                full_text.append(page_text)

        return "\n".join(full_text)

    elif ext == ".csv":
        df = pd.read_csv(file_path)
        return df.to_string(index=False)

    # åªæœ‰ cover åˆ° excel è£¡é¢çš„ç¬¬ä¸€å€‹å·¥ä½œè¡¨
    # å…¶é¤˜çš„å·¥ä½œè¡¨å…§å®¹ä¸æœƒè¢«è®€å–åˆ°
    elif ext == ".xlsx":
        df = pd.read_excel(file_path)
        return df.to_string(index=False)

    # å¦‚æœæ˜¯ HTML è£¡é¢æœ‰åœ–ç‰‡çš„è©±ï¼Œ <img src> è£¡é¢çš„ alt ä¹Ÿå°±æ˜¯åœ–ç‰‡çš„æ–‡å­—ç„¡æ³•é¡¯ç¤ºçš„æ™‚å€™æœƒè·³å‡ºçš„æ›¿ä»£æ–‡å­—ï¼Œæœƒè®€å–ä¸åˆ°ã€‚
    elif ext in [".html", ".xml"]:
        with open(file_path, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
        return soup.get_text()

    elif ext == ".json":
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return json.dumps(data, indent=2, ensure_ascii=False)

    else:
        raise ValueError(f"Unsupported file type: {ext}")


def save_as_txt(text: str, output_path: str):
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(text)


# ğŸš€ æ¸¬è©¦åŸ·è¡Œï¼šåªè¦æä¾› input_path
if __name__ == "__main__":
    input_path = input("è¼¸å…¥æª”æ¡ˆè·¯å¾‘ï¼š").strip()
    output_path = os.path.splitext(input_path)[0] + "_extracted.txt"

    try:
        content = extract_text(input_path)
        save_as_txt(content, output_path)
        print(f"âœ… æˆåŠŸå„²å­˜ç´”æ–‡å­—åˆ°ï¼š{output_path}")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤ï¼š{e}")
