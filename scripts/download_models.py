# scripts/download_models.py

"""
æ¨¡å‹ä¸‹è¼‰å’Œ ONNX è½‰æ›è…³æœ¬
æ”¯æ´å¾ Hugging Face ä¸‹è¼‰ BERT NER æ¨¡å‹ä¸¦è½‰æ›ç‚º ONNX æ ¼å¼
"""

import os
import sys
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import requests
from tqdm import tqdm
import torch
import onnx
import onnxruntime as ort
from onnxruntime.tools import optimizer
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    AutoConfig,
    GPT2LMHeadModel,
    GPT2Tokenizer
)
from huggingface_hub import hf_hub_download
from torch.onnx import export as torch_onnx_export


# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelDownloader:
    """æ¨¡å‹ä¸‹è¼‰å’Œè½‰æ›ç®¡ç†å™¨"""

    def __init__(self, base_model_dir: str = "models"):
        self.base_model_dir = Path(base_model_dir)
        self.base_model_dir.mkdir(exist_ok=True)

        # é è¨­æ¨¡å‹é…ç½®
        self.models_config = {
            "ner": {
                "model_name": "dbmdz/bert-large-cased-finetuned-conll03-english",
                "local_path": self.base_model_dir / "ner",
                "files": ["config.json", "pytorch_model.bin", "tokenizer_config.json", "vocab.txt"]
            },
            "ner_chinese": {
                "model_name": "ckiplab/bert-base-chinese-ner",
                "local_path": self.base_model_dir / "ner_chinese",
                "files": ["config.json", "pytorch_model.bin", "tokenizer_config.json", "vocab.txt"]
            },
            "gpt2": {
                "model_name": "gpt2",
                "local_path": self.base_model_dir / "gpt2",
                "files": ["config.json", "pytorch_model.bin", "tokenizer_config.json", "vocab.json", "merges.txt"]
            },
            "gpt2_chinese": {
                "model_name": "uer/gpt2-chinese-cluecorpussmall",
                "local_path": self.base_model_dir / "gpt2_chinese",
                "files": ["config.json", "pytorch_model.bin", "tokenizer_config.json", "vocab.json"]
            },
            "layout": {
                "model_name": "microsoft/layoutlmv3-base",
                "local_path": self.base_model_dir / "layout",
                "files": ["config.json", "pytorch_model.bin", "tokenizer_config.json"]
            }
        }

        # ONNX æ¨¡å‹ä¸‹è¼‰é…ç½®
        self.onnx_models = {
            "bert_ner_onnx": {
                "local_path": self.base_model_dir / "ner" / "model.onnx",
                "url": "https://huggingface.co/optimum/bert-base-NER/resolve/main/model.onnx"
            },
            "layout_onnx": {
                "local_path": self.base_model_dir / "layout" / "model.onnx",
                "url": "https://huggingface.co/microsoft/layoutlmv3-base-onnx/resolve/main/model.onnx"
            }
        }

    def download_model(self, model_key: str, force_download: bool = False) -> Dict[str, Path]:
        """
        ä¸‹è¼‰æŒ‡å®šæ¨¡å‹

        Args:
            model_key: æ¨¡å‹é…ç½®éµå€¼
            force_download: å¼·åˆ¶é‡æ–°ä¸‹è¼‰

        Returns:
            åŒ…å«æ¨¡å‹è·¯å¾‘çš„å­—å…¸
        """
        if model_key not in self.model_configs:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_key}")

        config = self.model_configs[model_key]
        model_name = config["model_name"]

        # è¨­å®šæœ¬åœ°è·¯å¾‘
        local_model_dir = self.base_model_dir / "ner" / model_key
        local_model_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"ä¸‹è¼‰æ¨¡å‹: {model_name}")

        if not force_download and self._check_model_exists(local_model_dir):
            logger.info(f"æ¨¡å‹å·²å­˜åœ¨: {local_model_dir}")
        else:
            # ä¸‹è¼‰ tokenizer
            logger.info("ä¸‹è¼‰ tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir=local_model_dir / "tokenizer"
            )
            tokenizer.save_pretrained(local_model_dir / "tokenizer")

            # ä¸‹è¼‰æ¨¡å‹
            logger.info("ä¸‹è¼‰æ¨¡å‹...")
            model = AutoModelForTokenClassification.from_pretrained(
                model_name,
                cache_dir=local_model_dir / "pytorch_model"
            )
            model.save_pretrained(local_model_dir / "pytorch_model")

            # ä¸‹è¼‰é…ç½®
            model_config = AutoConfig.from_pretrained(model_name)
            model_config.save_pretrained(local_model_dir / "config")

        return {
            "model_dir": local_model_dir,
            "tokenizer_dir": local_model_dir / "tokenizer",
            "pytorch_model_dir": local_model_dir / "pytorch_model",
            "config_dir": local_model_dir / "config"
        }

    def download_file(self, url: str, local_path: Path) -> bool:
        """ä¸‹è¼‰å–®å€‹æ–‡ä»¶"""
        try:
            local_path.parent.mkdir(parents=True, exist_ok=True)

            response = requests.get(url, stream=True)
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))

            with open(local_path, 'wb') as f, tqdm(
                desc=local_path.name,
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))

            logger.info(f"å·²ä¸‹è¼‰: {local_path}")
            return True

        except Exception as e:
            logger.error(f"ä¸‹è¼‰å¤±æ•— {url}: {e}")
            return False

    def download_hf_model(self, model_name: str, local_path: Path, files: List[str]) -> bool:
        """å¾ Hugging Face ä¸‹è¼‰æ¨¡å‹"""
        try:
            local_path.mkdir(parents=True, exist_ok=True)

            logger.info(f"é–‹å§‹ä¸‹è¼‰ {model_name} åˆ° {local_path}")

            for file_name in files:
                try:
                    file_path = hf_hub_download(
                        repo_id=model_name,
                        filename=file_name,
                        cache_dir=str(local_path.parent),
                        local_dir=str(local_path),
                        local_dir_use_symlinks=False
                    )
                    logger.info(f"å·²ä¸‹è¼‰: {file_name}")
                except Exception as e:
                    logger.warning(f"è·³éæ–‡ä»¶ {file_name}: {e}")

            # é©—è­‰é—œéµæ–‡ä»¶
            config_file = local_path / "config.json"
            if not config_file.exists():
                logger.error(f"ç¼ºå°‘é—œéµé…ç½®æ–‡ä»¶: {config_file}")
                return False

            logger.info(f"æ¨¡å‹ {model_name} ä¸‹è¼‰å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"ä¸‹è¼‰æ¨¡å‹ {model_name} å¤±æ•—: {e}")
            return False

    def convert_to_onnx(self, model_key: str, optimize: bool = True) -> Path:
        """
        å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚º ONNX æ ¼å¼

        Args:
            model_key: æ¨¡å‹é…ç½®éµå€¼
            optimize: æ˜¯å¦å„ªåŒ– ONNX æ¨¡å‹

        Returns:
            ONNX æ¨¡å‹è·¯å¾‘
        """
        if model_key not in self.model_configs:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_key}")

        config = self.model_configs[model_key]
        model_paths = self.download_model(model_key)

        # è¼‰å…¥æ¨¡å‹å’Œ tokenizer
        logger.info(f"è¼‰å…¥æ¨¡å‹é€²è¡Œ ONNX è½‰æ›: {model_key}")

        tokenizer = AutoTokenizer.from_pretrained(
            model_paths["tokenizer_dir"]
        )
        model = AutoModelForTokenClassification.from_pretrained(
            model_paths["pytorch_model_dir"]
        )
        model.eval()

        # æº–å‚™ç¤ºä¾‹è¼¸å…¥
        max_length = config["max_length"]
        dummy_text = "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ç”¨æ–¼ONNXè½‰æ›ã€‚" if config["language"] == "zh" else "This is a test text for ONNX conversion."

        inputs = tokenizer(
            dummy_text,
            max_length=max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # ONNX è¼¸å‡ºè·¯å¾‘
        onnx_model_dir = model_paths["model_dir"] / "onnx"
        onnx_model_dir.mkdir(exist_ok=True)
        onnx_path = onnx_model_dir / f"{model_key}.onnx"

        # åŸ·è¡Œè½‰æ›
        logger.info(f"è½‰æ›åˆ° ONNX: {onnx_path}")

        with torch.no_grad():
            torch_onnx_export(
                model,
                (inputs["input_ids"], inputs["attention_mask"]),
                str(onnx_path),
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=["input_ids", "attention_mask"],
                output_names=["logits"],
                dynamic_axes={
                    "input_ids": {0: "batch_size", 1: "sequence_length"},
                    "attention_mask": {0: "batch_size", 1: "sequence_length"},
                    "logits": {0: "batch_size", 1: "sequence_length"}
                }
            )

        # é©—è­‰ ONNX æ¨¡å‹
        self._validate_onnx_model(onnx_path, inputs)

        # å„ªåŒ–æ¨¡å‹
        if optimize:
            optimized_path = self._optimize_onnx_model(onnx_path)
            return optimized_path

        return onnx_path


    def _check_model_exists(self, model_dir: Path) -> bool:
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨"""
        required_dirs = ["tokenizer", "pytorch_model", "config"]
        return all((model_dir / dir_name).exists() for dir_name in required_dirs)

    def _validate_onnx_model(self, onnx_path: Path, sample_inputs: Dict[str, torch.Tensor]):
        """é©—è­‰ ONNX æ¨¡å‹"""
        logger.info("é©—è­‰ ONNX æ¨¡å‹...")

        # æª¢æŸ¥æ¨¡å‹çµæ§‹
        onnx_model = onnx.load(str(onnx_path))
        onnx.checker.check_model(onnx_model)

        # æ¸¬è©¦æ¨ç†
        session = ort.InferenceSession(str(onnx_path))

        ort_inputs = {
            "input_ids": sample_inputs["input_ids"].numpy(),
            "attention_mask": sample_inputs["attention_mask"].numpy()
        }

        outputs = session.run(None, ort_inputs)
        logger.info(f"ONNX æ¨¡å‹è¼¸å‡º shape: {outputs[0].shape}")
        logger.info("ONNX æ¨¡å‹é©—è­‰æˆåŠŸ!")

    def _optimize_onnx_model(self, onnx_path: Path) -> Path:
        """å„ªåŒ– ONNX æ¨¡å‹"""
        logger.info("å„ªåŒ– ONNX æ¨¡å‹...")

        optimized_path = onnx_path.with_suffix(".optimized.onnx")

        # åŸºæœ¬å„ªåŒ–
        optimizer.optimize_model(
            str(onnx_path),
            str(optimized_path),
            file_type="onnx"
        )

        logger.info(f"å„ªåŒ–å®Œæˆ: {optimized_path}")
        return optimized_path


    def verify_models(self) -> Dict[str, bool]:
        """é©—è­‰æ‰€æœ‰æ¨¡å‹æ˜¯å¦æ­£ç¢ºä¸‹è¼‰"""
        results = {}

        for model_key, config in self.models_config.items():
            local_path = config["local_path"]
            config_file = local_path / "config.json"

            if config_file.exists():
                try:
                    # å˜—è©¦è¼‰å…¥æ¨¡å‹é©—è­‰
                    if "ner" in model_key:
                        AutoTokenizer.from_pretrained(str(local_path))
                        results[model_key] = True
                    elif "gpt2" in model_key:
                        GPT2Tokenizer.from_pretrained(str(local_path))
                        results[model_key] = True
                    else:
                        results[model_key] = True

                    logger.info(f"âœ“ {model_key} é©—è­‰é€šé")

                except Exception as e:
                    logger.error(f"âœ— {model_key} é©—è­‰å¤±æ•—: {e}")
                    results[model_key] = False
            else:
                logger.error(f"âœ— {model_key} ä¸å­˜åœ¨: {config_file}")
                results[model_key] = False

        return results

    def download_all(self) -> bool:
        """ä¸‹è¼‰æ‰€æœ‰å¿…è¦çš„æ¨¡å‹"""
        logger.info("é–‹å§‹ä¸‹è¼‰ EdgeDeID Studio æ‰€éœ€æ¨¡å‹...")

        success_count = 0
        total_count = len(self.models_config)

        # ä¸‹è¼‰ Hugging Face æ¨¡å‹
        for model_key, config in self.models_config.items():
            if self.download_hf_model(
                config["model_name"],
                config["local_path"],
                config["files"]
            ):
                success_count += 1

        # ä¸‹è¼‰ ONNX æ¨¡å‹
        for onnx_key, config in self.onnx_models.items():
            if not config["local_path"].exists():
                if self.download_file(config["url"], config["local_path"]):
                    logger.info(f"ONNX æ¨¡å‹ä¸‹è¼‰å®Œæˆ: {onnx_key}")

        # è½‰æ› ONNX æ¨¡å‹
        self.convert_to_onnx("bert_ner")

        # é©—è­‰çµæœ
        verification_results = self.verify_models()
        verified_count = sum(verification_results.values())

        logger.info(f"æ¨¡å‹ä¸‹è¼‰å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")
        logger.info(f"æ¨¡å‹é©—è­‰å®Œæˆ: {verified_count}/{total_count} é€šé")

        if verified_count == total_count:
            logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹ä¸‹è¼‰ä¸¦é©—è­‰æˆåŠŸï¼")
            return True
        else:
            logger.warning("âš ï¸  éƒ¨åˆ†æ¨¡å‹ä¸‹è¼‰æˆ–é©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²çµ¡é€£æ¥å’Œç£ç›¤ç©ºé–“")
            return False

def main():
    """ä¸»å‡½æ•¸"""
    downloader = ModelDownloader()

    # æª¢æŸ¥ç¾æœ‰æ¨¡å‹
    logger.info("æª¢æŸ¥ç¾æœ‰æ¨¡å‹...")
    existing_results = downloader.verify_models()
    missing_models = [k for k, v in existing_results.items() if not v]

    if not missing_models:
        logger.info("âœ“ æ‰€æœ‰æ¨¡å‹å·²å­˜åœ¨ä¸”é©—è­‰é€šé")
        return True

    logger.info(f"éœ€è¦ä¸‹è¼‰çš„æ¨¡å‹: {missing_models}")

    # ä¸‹è¼‰ç¼ºå°‘çš„æ¨¡å‹
    success = downloader.download_all()

    if success:
        logger.info("æ¨¡å‹ä¸‹è¼‰æµç¨‹å®Œæˆï¼Œå¯ä»¥é–‹å§‹ä½¿ç”¨ EdgeDeID Studio")
    else:
        logger.error("æ¨¡å‹ä¸‹è¼‰æµç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ä¸¦é‡è©¦")
        sys.exit(1)

    return success

if __name__ == "__main__":
    main()


