# EdgeDeID Studio

EdgeDeID Studio is a real-time, on-device personal data anonymization toolkit that detects and redacts sensitive information (PII) from PDF documents, images, and tabular data within **150 ms**.

## âœ¨ Features

- ğŸ” **NER + OCR PII Detection**: Identifies names, emails, addresses, ID numbers, and more.
- ğŸ§  **Generative AI Augmentation**: Replace redacted info with synthetic names, or generate summaries.
- ğŸ“„ **Document Support**: Works with PDF, image, and CSV/Excel files.
- âš¡ **Edge-Optimized**: Quantized ONNX models run on Qualcomm Copilot+ NPU with <150ms latency.
- ğŸ›¡ï¸ **Privacy-First**: Everything runs locally. No data leaves the device.

## ğŸ§° Tech Stack

- **NER model**: `ckiplab/bert-base-chinese-ner`
- **Fake data generation**: `uer/gpt2-chinese-cluecorpussmall`
- **PDF/Image parsing**: `PyMuPDF`, `Pillow`, `pandas`
- **ONNX Inference**: `onnx`, `onnxruntime`, `onnxsim`
- **UI**: PySide6 (for graphical interface)

## ğŸ—‚ï¸ Project Structure

## PII Models
### ğŸ§° [Azure AI](https://learn.microsoft.com/zh-tw/azure/ai-services/language-service/personally-identifiable-information/overview?source=recommendations&tabs=text-pii) èªè¨€å€‹äººæ¨™è­˜è³‡è¨Š PII detection
Python ver: [Azure Python](https://learn.microsoft.com/zh-tw/azure/ai-services/language-service/personally-identifiable-information/quickstart?tabs=windows&pivots=programming-language-python)

- å¯ä»¥åƒè€ƒä½¿ç”¨ï¼Œä½†æ˜¯è¦æ”¶è²» -> å…ä»˜è²»ç‰ˆæœ¬æœƒæœ‰æ‡‰ç”¨ä¸Šçš„é™åˆ¶

### ğŸ§° [Better Data AI](https://huggingface.co/betterdataai/PII_DETECTION_MODEL)

- ä¸ç¢ºå®šå¥½ä¸å¥½ç”¨

```python
user_input = "Write an email to Julia indicating I won't be coming to office on the 29th of June"
new_prompt = prompt.format(classes="\n".join(classes_list) , text=user_input)
tokenized_input = tokenizer(new_prompt , return_tensors="pt").to(device)

output = model.generate(**tokenized_input , max_new_tokens=6000)
pii_classes = tokenizer.decode(output[0] , skip_special_tokens=True).split("The PII data are:\n")[1]

print(pii_classes)

##output
"""
<name> : ['Julia']
<date> : ['the 29th of June']
"""
```

### ğŸ§° [predidio](https://github.com/microsoft/presidio)
#### [Demo](https://huggingface.co/spaces/presidio/presidio_demo)

- Data Protection and De-identification SDK
- æ•ˆæœä½³

#### é›£é»
- å¤šç¨®èªè¨€é›£ä¸€æ¬¡åµæ¸¬(é™¤éç›´æ¥ä½¿ç”¨å¤šèª PII NER æ¨¡å‹åµæ¸¬)
- Spacy ä¸€æ¬¡åªèƒ½åµæ¸¬ä¸€ç¨®èªè¨€ (éœ€è¦å¤šæ¬¡å‘¼å« -> æ•ˆèƒ½ bad bad | ä½¿ç”¨è€…ç«¯é å…ˆé¸æ“‡ input file çš„èªè¨€)

### ğŸ§° [Multilingual NER](https://huggingface.co/Babelscape/wikineural-multilingual-ner)
- mBERT multilingual language model
- model is trained on WikiNEuRal (Therefore, it might not generalize well to all textual genres (e.g. news))

### ğŸ§° [xlm-roberta-base-ner-hrl](https://huggingface.co/Davlan/xlm-roberta-base-ner-hrl)
- based on a fine-tuned XLM-RoBERTa base model

### ğŸ§° [piiranha-v1-detect-personal-information](https://huggingface.co/iiiorg/piiranha-v1-detect-personal-information)
- open in Colab å¯ä»¥ç›´æ¥å¯¦æ¸¬
- 

### ğŸ” sensitive_data_generator

é€™å€‹å­æ¨¡çµ„è² è²¬ã€Œåˆæˆã€å¤šæ ¼å¼ã€å«æ•æ„Ÿè³‡æ–™çš„å‡æ¸¬è©¦æ–‡ä»¶ï¼Œä¾› De-ID pipeline æ¸¬è©¦èˆ‡ benchmarkã€‚

#### 2.1 `__init__.py`

```python
from .config import *
from .generators import PIIGenerator
from .formatters import DataFormatter
from .advanced_formatters import AdvancedDataFormatter
from .file_writers import FileWriter
from .advanced_file_writers import AdvancedFileWriter
from .dataset_generator import MultiFormatDatasetGenerator

__all__ = [
  "PIIGenerator", "DataFormatter", "FileWriter",
  "AdvancedDataFormatter","AdvancedFileWriter","MultiFormatDatasetGenerator"
]
````

* **åŠŸèƒ½**ï¼šæŠŠæ¨¡çµ„è£¡çš„æ ¸å¿ƒé¡åˆ¥ä¸€æ¬¡å°å‡º (`__all__`)ï¼Œæä¾›ä¸Šå±¤ `import sensitive_data_generator` å°±èƒ½æ‹¿åˆ°ç”¢ç”Ÿå™¨ã€æ ¼å¼å™¨ã€æª”æ¡ˆè¼¸å‡ºç­‰æ‰€æœ‰å·¥å…·ã€‚

#### 2.2 `advanced_file_writers.py`

```python
class AdvancedFileWriter:
    """é€²éšå¤šæ ¼å¼æª”æ¡ˆè¼¸å‡ºå·¥å…·"""

    @staticmethod
    def create_complex_pdf(content, output_dir, filename=None, include_charts=True):
        # 1. ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # 2. å»ºç«‹ ReportLab PDF æ–‡ä»¶
        doc = SimpleDocTemplate(filepath, pagesize=letter)
        styles = getSampleStyleSheet()
        elements = []

        # 3. åŠ æ¨™é¡Œèˆ‡æ­£æ–‡
        title = Paragraph("æ©Ÿå¯†æ–‡ä»¶ â€“ å€‹äººè³‡æ–™å ±å‘Š", styles['Heading1'])
        elements.append(title)
        elements.append(Spacer(1, 12))
        pii_para = Paragraph(content, styles['BodyText'])
        elements.append(pii_para)
        elements.append(Spacer(1, 12))

        # 4. åŠ è¡¨æ ¼ï¼ˆç¤ºç¯„æ’å…¥ 4 æ¬„ï¼šå§“åã€IDã€é›»è©±ã€åœ°å€ï¼‰
        table_data = [
          ['é …ç›®','åŸå§‹è³‡æ–™','å‚™è¨»'],
          ['å§“å', PIIGenerator.generate_tw_name(), 'æ¸¬è©¦ç”¨è™›æ“¬å§“å'],
          ['èº«åˆ†è­‰', PIIGenerator.generate_tw_id(), 'æ¸¬è©¦ç”¨è™›æ“¬ID'],
          ['é›»è©±', PIIGenerator.generate_tw_phone(), 'æ¸¬è©¦ç”¨è™›æ“¬é›»è©±'],
          ['åœ°å€', PIIGenerator.generate_tw_address(), 'æ¸¬è©¦ç”¨è™›æ“¬åœ°å€']
        ]
        table = Table(table_data, colWidths=[1.5*inch,3*inch,2.5*inch])
        table.setStyle(TableStyle([...]))
        elements.append(table)
        elements.append(Spacer(1, 24))

        # 5. å¯é¸ï¼šæ’å…¥å‡åœ–è¡¨ï¼Œåœ–ç”¨ PIL+matplotlib ç”Ÿæˆ
        if include_charts:
            chart_img = AdvancedFileWriter.generate_fake_chart()
            elements.append(RLImage(chart_img, width=5*inch, height=3*inch))
            elements.append(Paragraph("åœ–1ï¼šæ¸¬è©¦è³‡æ–™åˆ†ä½ˆåœ–", styles['Italic']))

        # 6. å¯«å‡º PDF
        doc.build(elements)
        return filepath
```

* **åŠŸèƒ½æ‹†è§£**

  1. **ç›®éŒ„æª¢æŸ¥**ï¼š`os.makedirs(...)`
  2. **PDF**ï¼šä½¿ç”¨ ReportLab `SimpleDocTemplate` + `Paragraph`ï¼‹`Table`ï¼‹`Spacer`
  3. **å‡è³‡æ–™è¡¨æ ¼**ï¼š`PIIGenerator` éš¨æ©Ÿç”Ÿæˆå§“åã€IDã€é›»è©±ã€åœ°å€
  4. **å‡åœ–è¡¨**ï¼šå‘¼å« `generate_fake_chart()` â†’ éš¨æ©Ÿç”¢ç”Ÿ bar/line/pie åœ–
  5. **åŒ¯å‡º**ï¼šå›å‚³å®Œæ•´æª”æ¡ˆè·¯å¾‘

```python
    @staticmethod
    def generate_fake_chart():
        """ç”Ÿæˆ Bar/Line/Pie å‡åœ–è¡¨"""
        plt.figure(figsize=(8,5))
        kind = random.choice(['bar','line','pie'])
        if kind=='bar':
            labels = ['Aéƒ¨é–€','Béƒ¨é–€','Céƒ¨é–€','Déƒ¨é–€']
            values = np.random.randint(100,500,size=4)
            plt.bar(labels, values)
            plt.title('éƒ¨é–€æ¥­ç¸¾æ¯”è¼ƒ')
        elif kind=='line':
            x = np.arange(1,11)
            y = np.random.rand(10)*100
            plt.plot(x,y,marker='o')
            plt.title('æœˆåº¦è¶¨å‹¢åˆ†æ')
        else:
            labels = ['é¡åˆ¥A','é¡åˆ¥B','é¡åˆ¥C','é¡åˆ¥D']
            sizes = np.random.randint(15,40,size=4)
            plt.pie(sizes, labels=labels, autopct='%1.1f%%')
            plt.title('é¡åˆ¥åˆ†ä½ˆåœ–')
        buf = io.BytesIO()
        plt.tight_layout()
        plt.savefig(buf, format='png', dpi=100)
        plt.close()
        buf.seek(0)
        return buf
```

* **åŠŸèƒ½**ï¼šç”¨ matplotlib éš¨æ©Ÿé¸æ“‡åœ–è¡¨é¡å‹ã€ç”Ÿæˆæ•¸æ“šå¾Œè¼¸å‡ºåˆ° `BytesIO`ï¼Œè®“ä¸Šå±¤ PDF/Word/PPTX éƒ½å¯ä»¥ç›´æ¥æ’åœ–ã€‚

> **å¾ŒçºŒ**ï¼š`create_word_document`ã€`create_powerpoint_presentation`ã€`create_excel_spreadsheet`ã€`create_scanned_document` éƒ½æ¡ç›¸åŒæ‹†åˆ†ï¼š
>
> * **Word** â†’ `python-docx`ï¼š`Document()`ã€`add_heading`ã€`add_table`ã€`add_picture`
> * **PPTX** â†’ `python-pptx`ï¼š`Presentation()`ã€`slides.add_slide()`ã€`shapes.add_table()`ã€`shapes.add_picture()`
> * **Excel** â†’ `pandas.DataFrame` + `ExcelWriter(engine='xlsxwriter')`ï¼›è¨­å®š header æ ¼å¼ã€æ¬„å¯¬ã€æ•¸å€¼æ ¼å¼
> * **æƒææª”** â†’ `PIL.ImageDraw`ï¼šç•«èƒŒæ™¯å™ªé»ã€æ–‡å­—ã€ç°½ç« ã€ç°½åï¼Œæ¨¡æ“¬æƒæå“è³ª

ä¸‹é¢ç¤ºç¯„å¦‚ä½•æŠŠ **`advanced_formatters.py`**ã€**`config.py`**ã€**`dataset_generator.py`** ä¹ŸåŒæ¨£è£œåˆ°æ–‡ä»¶è£¡ï¼Œä¸¦èªªæ˜æ¯å€‹å€å¡Šçš„åŠŸèƒ½èˆ‡ç›®çš„ã€‚


#### 2.3 `advanced_formatters.py`

```python
class AdvancedDataFormatter:
    """é€²éšè³‡æ–™æ ¼å¼åŒ–ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_contract_document():
        """
        ç”¢ç”Ÿä¸€ä»½åˆç´„åˆç´„ç¯„æœ¬ï¼ˆå«è™›æ“¬ç•¶äº‹äººè³‡æ–™ï¼‰ï¼š
        - parties: éš¨æ©Ÿç”¢ç”Ÿç”²ä¹™é›™æ–¹å§“åã€èº«åˆ†è­‰ã€åœ°å€ã€ç°½è¨‚æ—¥æœŸ
        - contract: å¡«å…¥å„æ¢æ¬¾æ¨£æ¿ï¼ˆç›®çš„ã€æœŸé™ã€å ±é…¬ã€ä¿å¯†ã€ç®¡è½„æ³•é™¢ç­‰ï¼‰
        """
        parties = {
          "ç”²æ–¹": PIIGenerator.generate_tw_name(),
          "ä¹™æ–¹": PIIGenerator.generate_tw_name(),
          "ç”²æ–¹èº«åˆ†è­‰": PIIGenerator.generate_tw_id(),
          "ä¹™æ–¹èº«åˆ†è­‰": PIIGenerator.generate_tw_id(),
          "ç”²æ–¹åœ°å€": PIIGenerator.generate_tw_address(),
          "ä¹™æ–¹åœ°å€": PIIGenerator.generate_tw_address(),
          "ç°½ç´„æ—¥æœŸ": (datetime.now() - timedelta(days=random.randint(1,365)))\
             .strftime("%Yå¹´%mæœˆ%dæ—¥")
        }
        contract = f"""
        åˆç´„æ›¸

        ç«‹åˆç´„ç•¶äº‹äººï¼š
        ç”²æ–¹ï¼š{parties['ç”²æ–¹']}ï¼ˆèº«åˆ†è­‰è™Ÿï¼š{parties['ç”²æ–¹èº«åˆ†è­‰']}ï¼‰
        ...
        ç¬¬å…­æ¢ ç®¡è½„æ³•é™¢  
        å› æœ¬åˆç´„ç™¼ç”Ÿä¹‹çˆ­è­°ï¼Œé›™æ–¹åŒæ„ä»¥å°ç£å°åŒ—åœ°æ–¹æ³•é™¢ç‚ºç¬¬ä¸€å¯©ç®¡è½„æ³•é™¢ã€‚

        ä¸­è¯æ°‘åœ‹ {parties['ç°½ç´„æ—¥æœŸ']}
        """
        return contract
````

* **åŠŸèƒ½**ï¼šç”¨ `PIIGenerator` éš¨æ©Ÿå¡«å…¥ã€Œåˆç´„ã€æ‰€éœ€é—œéµæ¬„ä½ï¼Œä¸¦é€éå¤šè¡Œå­—ä¸²æ¨¡æ¿ï¼ˆf-stringï¼‰çµ„æˆå®Œæ•´åˆç´„ç¯„æœ¬ã€‚

```python
    @staticmethod
    def generate_medical_report():
        """
        ç”Ÿæˆè©³ç´°é†«ç™‚å ±å‘Šæ–‡æœ¬ï¼ˆå«è™›æ“¬ç—…äººè³‡æ–™ + è™›æ“¬æª¢æŸ¥æ•¸æ“šï¼‰ï¼š
        - patient: éš¨æ©Ÿå§“åã€IDã€å‡ºç”Ÿã€é›»è©±ã€åœ°å€ã€ç—…æ­·è™Ÿ
        - test_results: è¡€å£“ã€å¿ƒç‡ã€è¡€ç³–ã€è†½å›ºé†‡ç­‰
        - report: f-string å¡«å…¥é†«é™¢åç¨±ã€å„ç¯€æ¨™é¡Œï¼ˆç—…å²ã€è¨ºæ–·ã€æª¢é©—ã€å½±åƒã€è™•æ–¹ã€é†«å›‘ï¼‰
        """
```

* **åŠŸèƒ½**ï¼šåŒæ¨£ç”¨ f-string + `HOSPITALS` åˆ—è¡¨éš¨æ©ŸæŒ‘é¸é†«é™¢ï¼Œçµ„å‡ºå¯ç›´æ¥è²¼æª”çš„é†«ç™‚å ±å‘Šæ¨¡æ¿ã€‚

---

#### 2.4 `config.py`

```python
# å°ç£åœ°å€å¸¸ç”¨åƒè€ƒè³‡æ–™ï¼Œä¾› Formatter/Generator ä½¿ç”¨
TAIWAN_LOCATIONS = {
  "åŒ—éƒ¨": ["å°åŒ—å¸‚","æ–°åŒ—å¸‚","åŸºéš†å¸‚",...],
  "ä¸­éƒ¨": ["å°ä¸­å¸‚","å½°åŒ–ç¸£",...],
  ...
}

STREET_NAMES = ["ä¸­å±±","ä¸­æ­£","å…‰å¾©",...]
SURNAMES     = ["é™³","æ—","å¼µ",...]
GIVEN_NAMES  = ["æ€¡å›","å¿—æ˜","é›…å©·",...]
HOSPITALS    = ["å°å¤§é†«é™¢","é•·åºšç´€å¿µé†«é™¢",...]
MEDICAL_SPECIALTIES = ["å…§ç§‘","å¤–ç§‘","å…’ç§‘",...]
```

* **åŠŸèƒ½**ï¼šæŠŠæ‰€æœ‰å¯éš¨æ©Ÿé¸ç”¨çš„åœ°åã€è¡—é“ã€å§“åã€é†«é™¢ã€ç§‘åˆ¥ç­‰åˆ—è¡¨é›†ä¸­ç®¡ç†ï¼Œæ–¹ä¾¿ Formatter å‘¼å«ã€‚

---

#### 2.5 `dataset_generator.py`

```python
class MultiFormatDatasetGenerator:
    """å¤šæ ¼å¼æ•æ„Ÿè³‡æ–™é›†ç”Ÿæˆå™¨"""

    @staticmethod
    def generate_full_dataset(output_dir, num_items=50):
        """
        ä¸€æ¬¡ç”Ÿç”¢å¤šç¨®æ ¼å¼ï¼ˆpdfã€wordã€imageã€excelã€pptã€contractsã€medicalã€financialâ€¦ï¼‰  
        - å»ºç«‹å­è³‡æ–™å¤¾ï¼špdf/ã€word/ã€scanned/ã€excel/ã€ppt/ã€contracts/ã€medical/ã€financial/  
        - é€ç­†å¾ªç’°ï¼šéš¨æ©Ÿé¸ contract/medical/financialï¼Œå‘¼å« AdvancedDataFormatter ç”¢æ–‡æœ¬  
        - å‘¼å« AdvancedFileWriter è¼¸å‡ºå°æ‡‰æ ¼å¼æª”æ¡ˆä¸¦ç´€éŒ„è·¯å¾‘  
        - æœ€å¾ŒåŒ¯å‡º metadata.jsonï¼ŒåŒ…å«æ¯ç­†çš„æ ¼å¼æ¸…å–®èˆ‡æª”æ¡ˆä½ç½®
        """
        # å»ºç›®éŒ„ã€åˆå§‹åŒ– dataset listâ€¦
        sub_dirs = {â€¦}
        for i in range(num_items):
          doc_type = random.choice(["contract","medical","financial"])
          if doc_type=="contract":
            content = AdvancedDataFormatter.generate_contract_document()
          elif doc_type=="medical":
            content = AdvancedDataFormatter.generate_medical_report()
          else:
            content = AdvancedDataFormatter.generate_financial_statement()

          pdf_path = AdvancedFileWriter.create_complex_pdf(content, sub_dirs["pdf"], f"{doc_type}_{i+1}.pdf")
          item["formats"].append({"format":"pdf","path":pdf_path})

          # â€¦åŒç†å‘¼å« create_word_documentã€create_scanned_document
          # è‹¥ financial é¡å¤–å‘¼å« create_excel_spreadsheetã€create_powerpoint_presentation

          # å¯« content .txtã€dataset.append(item)
        # å¯«å‡º dataset_metadata.json
```

* **åŠŸèƒ½**ï¼šæ•´åˆä»¥ä¸Š Formatter + FileWriterï¼Œæ‰¹æ¬¡ç”Ÿç”¢å¤šæ ¼å¼æ¸¬è©¦é›†ä¸¦è¼¸å‡º metadataï¼Œä¾¿æ–¼å¾ŒçºŒè‡ªå‹•åŒ–æ¸¬è©¦èˆ‡ benchmarkã€‚


---

### ğŸ› ï¸ Scripts utilities

### 1. `benchmark_formats.py` â€” æ ¼å¼æ•ˆèƒ½åŸºæº–æ¸¬è©¦
```python
from deid_pipeline import DeidPipeline
def benchmark_formats(dataset_dir, formats=["pdf","docx","xlsx","png"]):
    pipeline = DeidPipeline(language="zh")
    for fmt in formats:
        fmt_files = [f for f in os.listdir(dataset_dir) if f.endswith(fmt)]
        # æ¯ç¨®æ ¼å¼åªæ¸¬ 10 å€‹æª”æ¡ˆ
        for file in fmt_files[:10]:
            start = time.time()
            pipeline.process(os.path.join(dataset_dir, file))
            processing_times.append(time.time()-start)
````

* **åŠŸèƒ½**ï¼šå°æŒ‡å®šè³‡æ–™å¤¾ä¸­ï¼Œå„æ ¼å¼å‰10å€‹æª”æ¡ˆåšå»è­˜åˆ¥åŒ–ï¼Œæ”¶é›†åŸ·è¡Œæ™‚é–“ã€‚
* **ç”¨é€”**ï¼šé‡åŒ–ä¸åŒæª”æ¡ˆæ ¼å¼ï¼ˆPDFã€Wordã€Excelã€PNGï¼‰åœ¨å»è­˜åˆ¥åŒ–æµç¨‹ä¸­çš„å¹³å‡ï¼æœ€å°ï¼æœ€å¤§è™•ç†æ™‚é–“ï¼Œå¹«åŠ©èª¿å„ªèˆ‡è³‡æºè¦åŠƒã€‚

---

### 2. `download_models.py` â€” æ¨¡å‹é ä¸‹è¼‰

```python
MODELS = {
  "ner_zh": ("ckiplab/bert-base-chinese-ner", "models/ner/bert-ner-zh"),
  "gpt2_base": ("gpt2", "models/gpt2")
}
for name, (repo_id, target) in MODELS.items():
    # Transformers ä¸‹è¼‰ GPT-2
    if name=="gpt2_base" and not (Path(target)/"pytorch_model.bin").exists():
        tokenizer = AutoTokenizer.from_pretrained(repo_id)
        model = AutoModelForCausalLM.from_pretrained(repo_id)
        model.save_pretrained(target); tokenizer.save_pretrained(target)
    # HF Hub snapshot ä¸‹è¼‰ NER
    elif not Path(target).exists():
        snapshot_download(repo_id, local_dir=target)
```

* **åŠŸèƒ½**ï¼šè‡ªå‹•å¾ HuggingFace åŠ Transformers ä¸‹è¼‰ã€å¿«ç…§ä¿å­˜ BERT-NER èˆ‡ GPT-2 æ¨¡å‹åˆ° `models/`ã€‚
* **ç”¨é€”**ï¼šç¢ºä¿åœ˜éšŠä¸€éµåŸ·è¡Œæ™‚å·²å…·å‚™æœ¬åœ°æ¨¡å‹ï¼Œé¿å…é¦–æ¬¡é‹è¡Œæ™‚æ‰‹å‹•ä¸‹è¼‰å¤±æ•—ã€‚

---

### 3. `run_automated_pipeline.py` â€” è‡ªå‹•åŒ–æ¸¬è©¦ç®¡ç·š

```python
from deid_pipeline import DeidPipeline
def run_automated_test_pipeline(dataset_dir):
    pipeline = DeidPipeline(language="zh")
    for root, _, files in os.walk(dataset_dir):
        for fn in files:
            res = pipeline.process(os.path.join(root, fn))
            results.append({
                "file": fn,
                "format": fn.split(".")[-1],
                "pii_count": len(res.entities),
                "processing_time": res.processing_time
            })
    json.dump(results, open("pipeline_results.json","w"), ensure_ascii=False, indent=2)
```

* **åŠŸèƒ½**ï¼šéè¿´éæ­·è³‡æ–™é›†è³‡æ–™å¤¾ï¼Œå°æ¯æ”¯æª”æ¡ˆå‘¼å« `DeidPipeline.process()`ï¼Œä¸¦æŠŠ PII åµæ¸¬æ•¸ã€åŸ·è¡Œæ™‚é–“è¼¸å‡ºæˆ JSONã€‚
* **ç”¨é€”**ï¼šå¿«é€Ÿæª¢è¦–æ•´æ‰¹æ¸¬è©¦è³‡æ–™çš„å»è­˜åˆ¥åŒ–æˆæ•ˆï¼Œæ–¹ä¾¿ç”Ÿæˆå ±è¡¨æˆ–ä¸Šå‚³ CIã€‚

---

### 4. `validate_quality.py` â€” å»è­˜åˆ¥åŒ–å“è³ªé©—è­‰

```python
def validate_deidentification_quality(original_dir, processed_dir):
    for orig in os.listdir(original_dir):
        proc = os.path.join(processed_dir, orig)
        orig_text = open(os.path.join(original_dir,orig)).read()
        proc_text = open(proc).read()
        # æª¢æŸ¥æ˜¯å¦ç§»é™¤æ‰€æœ‰ PII
        for label in ["èº«åˆ†è­‰","é›»è©±","åœ°å€","ç—…æ­·è™Ÿ"]:
            if label in orig_text and label in proc_text:
                pii_removed=False
        quality_report.append({...})
    # è¨ˆç®—æˆåŠŸç‡
    pii_success = sum(r["pii_removed"] for r in quality_report)/len(quality_report)
    print(f"PII Removal Success: {pii_success:.2%}")
```

* **åŠŸèƒ½**ï¼šé€ä¸€æ¯”å°åŸæª”èˆ‡è™•ç†å¾Œæª”ï¼Œé©—è­‰ã€Œæ‰€æœ‰æ¨™è¨»çš„ PIIã€ç¢ºå¯¦æœªå‡ºç¾åœ¨å»è­˜åˆ¥åŒ–çµæœä¸­ï¼ŒåŒæ™‚å¯ç•™å¾…æ“´å……ã€Œè¡¨æ ¼ã€åœ–è¡¨å®Œæ•´æ€§æª¢æŸ¥ã€ã€‚
* **ç”¨é€”**ï¼šåœ¨ CICD æµç¨‹ä¸­è‡ªå‹•ç¢ºèªå»è­˜åˆ¥åŒ–è³ªé‡æŒ‡æ¨™ï¼ˆPII ç§»é™¤ç‡ã€æ ¼å¼ä¿ç•™ç‡ï¼‰ã€‚

---
