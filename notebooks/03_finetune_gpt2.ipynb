{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPO2Dj3gx3LHnRYNYNumnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin21523/edge-deid-studio/blob/feature%2Fadd-gpt2-finetune-notebook/notebooks/03_finetune_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3jHKrVpBB2S"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ['HF_HOME'] = '/content/drive/MyDrive/hf_cache'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/hf_cache/transformers'\n",
        "os.makedirs(os.environ['TRANSFORMERS_CACHE'], exist_ok=True)\n"
      ],
      "metadata": {
        "id": "A8EPBIhLBMWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass(\"請貼上你的 Hugging Face token：\")\n",
        "login(token=hf_token)\n"
      ],
      "metadata": {
        "id": "hldOqHLnBNWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "oL-4mmRkBO6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "邏輯說明\n",
        "這幾個 cell 與 02_train_ner.ipynb 如出一轍：第一步把 Transformers、Datasets、Accelerate 安裝好，第二步把快取搬到 Drive，第三步登入 Hugging Face（為了後續上傳模型或下載 private model），最後確認是否偵測到 GPU。雖然我們是先在免費版 Colab 上編輯，但請確保程式碼無誤，一旦切到 Pro 或有 GPU 時，就能直接跑訓練。"
      ],
      "metadata": {
        "id": "82twR4AMBRdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 二、下載與檢視資料集（Cell 5）"
      ],
      "metadata": {
        "id": "TfGLY6yJBXAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "print(raw_datasets)"
      ],
      "metadata": {
        "id": "Y_RFSMgMBXYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**做了什麼**\n",
        "使用 `load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")` 下載 WikiText-2 原始文字資料集，這是一個常用的小型語言模型微調語料。可以看到 train、validation、test 三個 split。"
      ],
      "metadata": {
        "id": "HzMwSyDzBcqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 三、Tokenizer 與 Model 載入（Cell 6）"
      ],
      "metadata": {
        "id": "Dv5iX27kBfcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_checkpoint = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "G4lxDzqCBjB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**為什麼這麼做**\n",
        "`gpt2` 預訓練模型本身就已經是 causal LM（自回歸），我們只要把 tokenizer 與 model 載下來，後續再把語料接在後面繼續調整權重。\n"
      ],
      "metadata": {
        "id": "i38abSKyBlV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 四、分段 Tokenize（Cell 7–8）"
      ],
      "metadata": {
        "id": "5XB733pxBnKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples):\n",
        "    # 把所有行合併成一個長字串，再依 block_size 切段落\n",
        "    concatenated = tokenizer.tokenize(\" \".join(examples[\"text\"]))\n",
        "    total_length = len(concatenated)\n",
        "    # 每段長度＝block_size 的倍數\n",
        "    block_size = 128\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        \"input_ids\": [concatenated[i : i + block_size] for i in range(0, total_length, block_size)],\n",
        "        \"attention_mask\": [[1] * block_size] * (total_length // block_size),\n",
        "    }\n",
        "    return result\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    lambda examples: tokenizer(examples[\"text\"]),\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ").map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_pGhjBprBo-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**背後邏輯**\n",
        "因為 GPT-2 是自回歸模型，需要固定長度的句子段落（block）來預測下一個 token。這裡先用 `tokenizer` 將每一句切成 token id，接著把所有 token 拼接後，依照 `block_size`（128）切成多個 chunk。每個 chunk 都會是 model 的一個訓練樣本。"
      ],
      "metadata": {
        "id": "7ERKjW05BqXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 五、DataCollator 與 Trainer 設定（Cell 9–11）"
      ],
      "metadata": {
        "id": "hyeRu2phBsu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False  # 因為 GPT-2 是自回歸 (causal LM)，不用做遮罩填空\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"models/gpt2/v1.0\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=200,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "N4YvhcLsBybb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**說明**\n",
        "\n",
        "* **DataCollatorForLanguageModeling**：自回歸時，不要遮罩 (mlm=False)。\n",
        "* **TrainingArguments**：設定 epoch、batch size、learning rate、權重衰減等等。\n",
        "* **Trainer**：把 model、dataset、collator、args 統整，之後只要呼叫 `trainer.train()` 就會跑微調。"
      ],
      "metadata": {
        "id": "LQ_J0jebB0O6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 六、啟動訓練（Cell 12）"
      ],
      "metadata": {
        "id": "rmP3jQC4B1hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 切到 GPU runtime 後才執行\n",
        "# trainer.train()\n",
        "# trainer.save_model()"
      ],
      "metadata": {
        "id": "7sgidjNSB3OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "這裡我們先把 `trainer.train()` 和 `trainer.save_model()` 註解掉，確保整段程式碼可以先跑過 syntax check。等到你切到有 GPU 的環境（或 Colab Pro）時，再把註解移除，一次跑完微調並儲存模型到 `models/gpt2/v1.0`。\n"
      ],
      "metadata": {
        "id": "TdoF20RrB4ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 小結\n",
        "\n",
        "目前 `03_finetune_gpt2.ipynb` 的流程已經完整：\n",
        "\n",
        "1. 安裝並設定環境\n",
        "2. 下載 WikiText-2 語料\n",
        "3. 載入 GPT-2 tokenizer & model\n",
        "4. 將文字切成固定長度的 block\n",
        "5. 用 `Trainer` 設定 fine-tune 超參\n",
        "6. （待 GPU）呼叫訓練並儲存"
      ],
      "metadata": {
        "id": "A1C9xZZEB7c3"
      }
    }
  ]
}