{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Dataset Pipeline (Prepare \u2192 Mix \u2192 Train \u2192 Export \u2192 Validate \u2192 Benchmark)\n",
        "\n",
        "This notebook runs the re-runnable dev-only pipeline script: `scripts/run_multi_dataset_pipeline.py`.\n",
        "\n",
        "What it does:\n",
        "- Prepares real Hugging Face datasets into local span-JSONL (text + gold entities)\n",
        "- Mixes multiple prepared datasets into a single training set\n",
        "- Fine-tunes a token-classification model\n",
        "- Exports to ONNX, validates parity, optionally INT8-quantizes, and benchmarks\n",
        "\n",
        "Notes:\n",
        "- The first run typically needs `--allow-network` to download datasets and (optionally) the base model.\n",
        "- Subsequent runs can be offline once caches are populated.\n",
        "- Outputs follow the AI_WAREHOUSE 3.0 layout under `/mnt/c` (cache/models) and `/mnt/data` (datasets/training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# AI_WAREHOUSE 3.0 cache layout (avoid $HOME/.cache)\n",
        "os.environ.setdefault('EDGE_DEID_CACHE_HOME', '/mnt/c/ai_cache')\n",
        "os.environ.setdefault('EDGE_DEID_MODELS_HOME', '/mnt/c/ai_models')\n",
        "os.environ.setdefault('EDGE_DEID_DATA_HOME', '/mnt/data')\n",
        "\n",
        "os.environ.setdefault('HF_HOME', '/mnt/c/ai_cache/huggingface')\n",
        "os.environ.setdefault('TRANSFORMERS_CACHE', os.environ['HF_HOME'])\n",
        "os.environ.setdefault('TORCH_HOME', '/mnt/c/ai_cache/torch')\n",
        "os.environ.setdefault('XDG_CACHE_HOME', '/mnt/c/ai_cache')\n",
        "os.environ.setdefault('PIP_CACHE_DIR', '/mnt/c/ai_cache/pip')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# End-to-end run using real datasets (requires network on first run).\n",
        "# Use --config to keep the run reproducible.\n",
        "!PYTHONPATH=src python scripts/run_multi_dataset_pipeline.py \\\n",
        "  --config configs/training/multi_zh_ner_demo.yaml \\\n",
        "  --allow-network \\\n",
        "  --trust-remote-code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "report_path = Path('/mnt/data/training/logs/edge_deid/multi-zh-ner-demo/report.json')\n",
        "report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "\n",
        "print('ONNX model:', report.get('onnx_model'))\n",
        "print('INT8 model:', report.get('onnx_model_int8'))\n",
        "print('Benchmark:', report.get('benchmark_onnx_ner', {}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Offline re-run (after the first run populated caches and prepared JSONL).\n",
        "!PYTHONPATH=src python scripts/run_multi_dataset_pipeline.py \\\n",
        "  --config configs/training/multi_zh_ner_demo.yaml \\\n",
        "  --json-out /mnt/data/training/logs/edge_deid/multi-zh-ner-demo/report.offline.json\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
