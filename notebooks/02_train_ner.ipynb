{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLlppiXwWeObV3UFV/C0TI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin21523/edge-deid-studio/blob/feature%2Ftrain-ner-notebook/notebooks/02_train_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🏷️ NER 微調示範（02_train_ner.ipynb）\n",
        "\n",
        "> 本筆記本示範如何下載資料、Tokenize、對齊標籤、以 Trainer 進行 NER 模型微調，並把結果儲存到 `models/ner/v1.0`。  \n",
        "> **注意**：先在 CPU 環境下 dry run，確認無誤後再切到 GPU Runtime 一鍵執行整個流程。\n",
        "\n",
        "## 1️⃣ 環境準備"
      ],
      "metadata": {
        "id": "xPoalFDO88nR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EuLYbuL7c0I"
      },
      "outputs": [],
      "source": [
        "# 1.1 安裝依賴（只需跑一次）\n",
        "!pip install -q transformers datasets accelerate\n",
        "\n",
        "# 1.2 掛載 Google Drive 並設定 HF 快取\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ['HF_HOME'] = '/content/drive/MyDrive/hf_cache'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/hf_cache/transformers'\n",
        "os.makedirs(os.environ['TRANSFORMERS_CACHE'], exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.3 Hugging Face 登入（第一次 runtime 必跑）\n",
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "hf_token = getpass(\"請貼上你的 Hugging Face token：\")\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "ntvKoFKa9ESN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.4 檢查 GPU（切到 GPU Runtime 再跑）\n",
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "7cJz7Kh29HMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2️⃣ 載入套件與資料集\n"
      ],
      "metadata": {
        "id": "q25Z2ymj9KaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "# 2.1 下載 Conll2003 NER 資料集\n",
        "raw_datasets = load_dataset(\"conll2003\")\n",
        "label_list   = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
        "num_labels   = len(label_list)\n",
        "print(\"Label 列表：\", label_list)\n"
      ],
      "metadata": {
        "id": "7lHViPiW9MJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3️⃣ Tokenizer 與 Model 初始化\n"
      ],
      "metadata": {
        "id": "58l_IodJ9Oe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer        = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model            = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "ilxh9vKh9P5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 定義對齊函式\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "    all_labels     = examples[\"ner_tags\"]\n",
        "    aligned_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids          = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids         = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(labels[word_idx])\n",
        "            else:\n",
        "                label_ids.append(labels[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        aligned_labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# 4.2 map 整個 dataset\n",
        "label_all_tokens   = False\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n"
      ],
      "metadata": {
        "id": "4nyVyg079RXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5️⃣ DataCollator 與 Trainer 設定\n"
      ],
      "metadata": {
        "id": "dwVTzpjl9Vg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 動態 padding\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# 5.2 訓練參數\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"models/ner/v1.0\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# 5.3 初始化 Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hYh8vg7V9XGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6️⃣ 開始訓練（待切到 GPU Runtime 再執行）\n"
      ],
      "metadata": {
        "id": "FnLq8Vfk9Y3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()、trainer.save_model() 都會把結果存到 models/ner/v1.0\n",
        "# 在 CPU 環境下可以先註解掉，GPU 一次跑完：\n",
        "# trainer.train()\n",
        "# trainer.save_model()"
      ],
      "metadata": {
        "id": "rCkXrkVh9bmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 📌 接下來的 Notebook 規劃\n",
        "\n",
        "- **02_train_ner.ipynb**：Conll2003 NER 微調示範  \n",
        "- **03_finetune_gpt2.ipynb**：GPT-2 自回歸微調  \n",
        "- **04_inference.ipynb**：載入微調後模型做推理展示  \n",
        "- **05_export_onnx.ipynb**：將微調後模型轉 ONNX／edge_models  \n",
        "- **06_evaluate.ipynb**：模型評估與指標視覺化\n",
        "\n",
        "所有 notebooks 都放在 `notebooks/`，每個加上 badge → Colab → Save to GitHub → 同步到 `notebooks/` 資料夾底下。\n"
      ],
      "metadata": {
        "id": "UHtZoYb09gdL"
      }
    }
  ]
}