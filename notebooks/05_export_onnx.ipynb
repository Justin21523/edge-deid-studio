{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9GW6A4bIRm/hDHXiyX4PA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin21523/edge-deid-studio/blob/feature%2Fadd-gpt2-onnx-export-notebook/notebooks/05_export_onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ä¸€ã€ç’°å¢ƒæº–å‚™ï¼ˆCell 1â€“3ï¼‰"
      ],
      "metadata": {
        "id": "rTXinrFvIsqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJZ_UjlyIqN0"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers onnx onnxruntime onnxruntime-tools accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ['HF_HOME']         = '/content/drive/MyDrive/hf_cache'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/hf_cache/transformers'\n",
        "os.makedirs(os.environ['TRANSFORMERS_CACHE'], exist_ok=True)"
      ],
      "metadata": {
        "id": "yJF1gsqXI7ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "PI1rrY0EI-bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ç‚ºä»€éº¼**ï¼š\n",
        ">\n",
        "> 1. å®‰è£ `onnx` èˆ‡ `onnxruntime`ã€`onnxruntime-tools` ä¾†æ”¯æ´ ONNX æ ¼å¼çš„åŒ¯å‡ºèˆ‡é©—è­‰ã€‚\n",
        "> 2. ç¢ºä¿æ‰€æœ‰ Hugging Face çš„å¿«å–éƒ½æ”¾åˆ° Driveï¼Œä¸æœƒå› ç‚ºé‡å•Ÿ Colab è€Œéºå¤±ã€‚\n",
        "> 3. ç¢ºèª GPU ç‹€æ…‹ï¼Œé›–ç„¶åŒ¯å‡º ONNX å¯ä»¥è·‘åœ¨ CPUï¼Œä½†å¾Œé¢è‹¥è¦åšé‡åŒ–ã€è½‰ Qualcomm DLC æ™‚å¯èƒ½éœ€è¦ GPUã€‚"
      ],
      "metadata": {
        "id": "Cc4oNtAwJFMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## äºŒã€è¼‰å…¥å¾®èª¿å¾Œçš„ GPT-2ï¼ˆCell 4ï¼‰"
      ],
      "metadata": {
        "id": "RhO1-vSoJITA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_dir = \"models/gpt2/v1.0\"       # èˆ‡ 03_finetune_gpt2.ipynb çš„ output_dir ç›¸åŒ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model     = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "model.eval()                         # åˆ‡åˆ°æ¨ç†æ¨¡å¼ï¼Œé—œé–‰ dropout"
      ],
      "metadata": {
        "id": "p_z5_aiUJHmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ç‚ºä»€éº¼**ï¼š\n",
        "> è¦æŠŠå‰›æ‰ fine-tune å®Œçš„æ¨¡å‹è¼‰å…¥ï¼Œæ‰èƒ½æŠŠå®ƒè½‰æˆ ONNXã€‚\n"
      ],
      "metadata": {
        "id": "YhjTshF_JPal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ä¸‰ã€åŒ¯å‡º ONNXï¼ˆCell 5ï¼‰"
      ],
      "metadata": {
        "id": "_AvlPbHYJSah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 1. æº–å‚™è¼¸å‡ºç›®éŒ„\n",
        "onnx_path = \"edge_models/gpt2/v1.0/gpt2.onnx\"\n",
        "os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n",
        "\n",
        "# 2. éš¨æ©Ÿ sample ä¸€çµ„ token åšç‚ºç¯„ä¾‹è¼¸å…¥\n",
        "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
        "\n",
        "# 3. ONNX exportï¼ˆåœ¨æœ‰ GPU æ™‚å¯åŒæ¨£åŸ·è¡Œåœ¨ CPUï¼‰\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
        "        onnx_path,\n",
        "        opset_version=13,\n",
        "        do_constant_folding=True,\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"logits\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "            \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "            \"logits\": {0: \"batch_size\", 1: \"seq_len\"}\n",
        "        }\n",
        "    )\n",
        "print(\"âœ… ONNX model saved to\", onnx_path)"
      ],
      "metadata": {
        "id": "fqP6bSA8JVJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **åŸç†**ï¼š\n",
        ">\n",
        "> * `torch.onnx.export` æœƒæŠŠ PyTorch æ¨¡å‹ graph è½‰æˆ ONNX æ ¼å¼ã€‚\n",
        "> * `opset_version=13` æ˜¯è¼ƒæ–°çš„ ONNX ç‰ˆæœ¬ï¼Œæ”¯æ´ Transformer å¸¸ç”¨ç®—å­ã€‚\n",
        "> * `dynamic_axes` å…è¨± batch sizeã€åºåˆ—é•·åº¦åœ¨æ¨ç†æ™‚å‹•æ…‹æ”¹è®Šã€‚\n",
        "> * `do_constant_folding=True` æœƒæŠŠ graph ä¸­å¯åˆä½µçš„å¸¸æ•¸é‹ç®—å…ˆåšæ‰ï¼ŒåŠ é€Ÿå¾ŒçºŒæ¨ç†ã€‚"
      ],
      "metadata": {
        "id": "iHtu2-eWJb66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å››ã€å‹•æ…‹é‡åŒ– (Int8 Quantization)ï¼ˆCell 6ï¼‰"
      ],
      "metadata": {
        "id": "BlAuhA-jJing"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "onnx_int8 = onnx_path.replace(\".onnx\", \"_int8.onnx\")\n",
        "quantize_dynamic(\n",
        "    onnx_path,\n",
        "    onnx_int8,\n",
        "    weight_type=QuantType.QInt8\n",
        ")\n",
        "print(\"âœ… Quantized ONNX model saved to\", onnx_int8)"
      ],
      "metadata": {
        "id": "c6RuJuUpJhnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ç‚ºä»€éº¼**ï¼š\n",
        ">\n",
        "> * Int8 é‡åŒ–å¯å¤§å¹…æ¸›å°‘æ¨¡å‹å¤§å°èˆ‡è¨˜æ†¶é«”éœ€æ±‚ï¼Œé©åˆé‚Šç·£è£ç½®ã€‚\n",
        "> * `quantize_dynamic` åªå°æ¬Šé‡åšé‡åŒ–ï¼Œä¸å½±éŸ¿åŸæœ¬ graph çµæ§‹ï¼Œé€Ÿåº¦èˆ‡ç²¾åº¦é€šå¸¸å¹³è¡¡è‰¯å¥½ã€‚"
      ],
      "metadata": {
        "id": "8-fTt8SOJk0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## äº”ã€è½‰æˆ Qualcomm Edge (DLC) æ ¼å¼ï¼ˆCell 7ï¼‰"
      ],
      "metadata": {
        "id": "7W2icYFxJoD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# å¦‚æœä½ å·²å®‰è£ Qualcomm SNPE SDKï¼Œå°±å¯ä»¥ç”¨ä»¥ä¸‹æŒ‡ä»¤è½‰ DLCï¼š\n",
        "# snpe-onnx-to-dlc \\\n",
        "#   --input_network edge_models/gpt2/v1.0/gpt2_int8.onnx \\\n",
        "#   --output_network edge_models/gpt2/v1.0/gpt2.dlc \\\n",
        "#   --input_dim input_ids:1,128 \\\n",
        "#   --input_dim attention_mask:1,128\n",
        "\n",
        "# è½‰å®Œä¹‹å¾Œï¼Œä½ å°±æœƒåœ¨ edge_models/gpt2/v1.0/ åº•ä¸‹çœ‹åˆ° gpt2.dlc"
      ],
      "metadata": {
        "id": "Ex2E5IgSKC-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **åŸç†**ï¼š\n",
        ">\n",
        "> * Qualcomm çš„ SNPE å·¥å…·èƒ½æŠŠ ONNX è½‰æˆè‡ªå®¶å°ˆç”¨çš„ `.dlc` æ ¼å¼ï¼Œæ–¹ä¾¿éƒ¨ç½²åˆ° Snapdragon NPUã€‚\n",
        "> * `--input_dim` ç”¨ä¾†ç¡¬ç·¨è¼¸å…¥ tensor shapeï¼ŒSNPE éœ€è¦çŸ¥é“æœ€å¤§ç¶­åº¦æ‰èƒ½åšå„ªåŒ–ã€‚"
      ],
      "metadata": {
        "id": "dG8GMn_fKHKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å…­ã€å°çµï¼ˆCell 8ï¼‰"
      ],
      "metadata": {
        "id": "jE7l6-wIKIsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸš€ Export pipeline complete!\")\n",
        "print(\"Check the following files under edge_models/gpt2/v1.0/:\")\n",
        "print(\"  â€¢ gpt2.onnx\")\n",
        "print(\"  â€¢ gpt2_int8.onnx\")\n",
        "print(\"  â€¢ gpt2.dlc   (å¦‚æœå·²åŸ·è¡Œ SNPE è½‰æª”)\")"
      ],
      "metadata": {
        "id": "eDfo6RBoKLrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å®Œæ•´æµç¨‹å›é¡§\n",
        "\n",
        "1. **å®‰è£ç’°å¢ƒ**ï¼šTransformers + ONNX å·¥å…·\n",
        "2. **è¼‰å…¥æ¨¡å‹**ï¼šå¾ `models/gpt2/v1.0` è®€å…¥å¾®èª¿å¾Œæ¬Šé‡\n",
        "3. **åŒ¯å‡º ONNX**ï¼š`torch.onnx.export` + `dynamic_axes`\n",
        "4. **é‡åŒ–**ï¼šå°‡ ONNX æ¬Šé‡è½‰ç‚º Int8ï¼Œæ¸›å°‘æ¨¡å‹å¤§å°\n",
        "5. **Qualcomm DLC**ï¼šï¼ˆé¸åšï¼‰ä½¿ç”¨ SNPE SDK æŠŠé‡åŒ–å¾Œçš„ ONNX è½‰æˆ `.dlc`"
      ],
      "metadata": {
        "id": "9p_Exw4-KRJI"
      }
    }
  ]
}