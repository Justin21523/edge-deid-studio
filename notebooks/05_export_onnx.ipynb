{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9GW6A4bIRm/hDHXiyX4PA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin21523/edge-deid-studio/blob/feature%2Fadd-gpt2-onnx-export-notebook/notebooks/05_export_onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 一、環境準備（Cell 1–3）"
      ],
      "metadata": {
        "id": "rTXinrFvIsqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJZ_UjlyIqN0"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers onnx onnxruntime onnxruntime-tools accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ['HF_HOME']         = '/content/drive/MyDrive/hf_cache'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/hf_cache/transformers'\n",
        "os.makedirs(os.environ['TRANSFORMERS_CACHE'], exist_ok=True)"
      ],
      "metadata": {
        "id": "yJF1gsqXI7ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "PI1rrY0EI-bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **為什麼**：\n",
        ">\n",
        "> 1. 安裝 `onnx` 與 `onnxruntime`、`onnxruntime-tools` 來支援 ONNX 格式的匯出與驗證。\n",
        "> 2. 確保所有 Hugging Face 的快取都放到 Drive，不會因為重啟 Colab 而遺失。\n",
        "> 3. 確認 GPU 狀態，雖然匯出 ONNX 可以跑在 CPU，但後面若要做量化、轉 Qualcomm DLC 時可能需要 GPU。"
      ],
      "metadata": {
        "id": "Cc4oNtAwJFMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 二、載入微調後的 GPT-2（Cell 4）"
      ],
      "metadata": {
        "id": "RhO1-vSoJITA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_dir = \"models/gpt2/v1.0\"       # 與 03_finetune_gpt2.ipynb 的 output_dir 相同\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model     = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "model.eval()                         # 切到推理模式，關閉 dropout"
      ],
      "metadata": {
        "id": "p_z5_aiUJHmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **為什麼**：\n",
        "> 要把剛才 fine-tune 完的模型載入，才能把它轉成 ONNX。\n"
      ],
      "metadata": {
        "id": "YhjTshF_JPal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 三、匯出 ONNX（Cell 5）"
      ],
      "metadata": {
        "id": "_AvlPbHYJSah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 1. 準備輸出目錄\n",
        "onnx_path = \"edge_models/gpt2/v1.0/gpt2.onnx\"\n",
        "os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n",
        "\n",
        "# 2. 隨機 sample 一組 token 做為範例輸入\n",
        "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
        "\n",
        "# 3. ONNX export（在有 GPU 時可同樣執行在 CPU）\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
        "        onnx_path,\n",
        "        opset_version=13,\n",
        "        do_constant_folding=True,\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"logits\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "            \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
        "            \"logits\": {0: \"batch_size\", 1: \"seq_len\"}\n",
        "        }\n",
        "    )\n",
        "print(\"✅ ONNX model saved to\", onnx_path)"
      ],
      "metadata": {
        "id": "fqP6bSA8JVJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **原理**：\n",
        ">\n",
        "> * `torch.onnx.export` 會把 PyTorch 模型 graph 轉成 ONNX 格式。\n",
        "> * `opset_version=13` 是較新的 ONNX 版本，支援 Transformer 常用算子。\n",
        "> * `dynamic_axes` 允許 batch size、序列長度在推理時動態改變。\n",
        "> * `do_constant_folding=True` 會把 graph 中可合併的常數運算先做掉，加速後續推理。"
      ],
      "metadata": {
        "id": "iHtu2-eWJb66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 四、動態量化 (Int8 Quantization)（Cell 6）"
      ],
      "metadata": {
        "id": "BlAuhA-jJing"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "onnx_int8 = onnx_path.replace(\".onnx\", \"_int8.onnx\")\n",
        "quantize_dynamic(\n",
        "    onnx_path,\n",
        "    onnx_int8,\n",
        "    weight_type=QuantType.QInt8\n",
        ")\n",
        "print(\"✅ Quantized ONNX model saved to\", onnx_int8)"
      ],
      "metadata": {
        "id": "c6RuJuUpJhnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **為什麼**：\n",
        ">\n",
        "> * Int8 量化可大幅減少模型大小與記憶體需求，適合邊緣裝置。\n",
        "> * `quantize_dynamic` 只對權重做量化，不影響原本 graph 結構，速度與精度通常平衡良好。"
      ],
      "metadata": {
        "id": "8-fTt8SOJk0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 五、轉成 Qualcomm Edge (DLC) 格式（Cell 7）"
      ],
      "metadata": {
        "id": "7W2icYFxJoD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 如果你已安裝 Qualcomm SNPE SDK，就可以用以下指令轉 DLC：\n",
        "# snpe-onnx-to-dlc \\\n",
        "#   --input_network edge_models/gpt2/v1.0/gpt2_int8.onnx \\\n",
        "#   --output_network edge_models/gpt2/v1.0/gpt2.dlc \\\n",
        "#   --input_dim input_ids:1,128 \\\n",
        "#   --input_dim attention_mask:1,128\n",
        "\n",
        "# 轉完之後，你就會在 edge_models/gpt2/v1.0/ 底下看到 gpt2.dlc"
      ],
      "metadata": {
        "id": "Ex2E5IgSKC-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **原理**：\n",
        ">\n",
        "> * Qualcomm 的 SNPE 工具能把 ONNX 轉成自家專用的 `.dlc` 格式，方便部署到 Snapdragon NPU。\n",
        "> * `--input_dim` 用來硬編輸入 tensor shape，SNPE 需要知道最大維度才能做優化。"
      ],
      "metadata": {
        "id": "dG8GMn_fKHKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 六、小結（Cell 8）"
      ],
      "metadata": {
        "id": "jE7l6-wIKIsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🚀 Export pipeline complete!\")\n",
        "print(\"Check the following files under edge_models/gpt2/v1.0/:\")\n",
        "print(\"  • gpt2.onnx\")\n",
        "print(\"  • gpt2_int8.onnx\")\n",
        "print(\"  • gpt2.dlc   (如果已執行 SNPE 轉檔)\")"
      ],
      "metadata": {
        "id": "eDfo6RBoKLrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 完整流程回顧\n",
        "\n",
        "1. **安裝環境**：Transformers + ONNX 工具\n",
        "2. **載入模型**：從 `models/gpt2/v1.0` 讀入微調後權重\n",
        "3. **匯出 ONNX**：`torch.onnx.export` + `dynamic_axes`\n",
        "4. **量化**：將 ONNX 權重轉為 Int8，減少模型大小\n",
        "5. **Qualcomm DLC**：（選做）使用 SNPE SDK 把量化後的 ONNX 轉成 `.dlc`"
      ],
      "metadata": {
        "id": "9p_Exw4-KRJI"
      }
    }
  ]
}