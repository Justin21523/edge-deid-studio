{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation and Benchmarking\\n",
        "\\n",
        "This notebook shows how to benchmark the pipeline and ONNX detector and how to enable perf regression tests.\\n",
        "\\n",
        "Notes:\\n",
        "- Benchmarks are best-effort: they will fail fast if a required local model file is missing.\\n",
        "- Perf regression tests are opt-in via `RUN_PERF_TESTS=1`.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\\n",
        "from pathlib import Path\\n",
        "\\n",
        "# AI_WAREHOUSE 3.0 cache layout (avoid $HOME/.cache)\\n",
        "os.environ.setdefault('HF_HOME', '/mnt/c/ai_cache/huggingface')\\n",
        "os.environ.setdefault('TRANSFORMERS_CACHE', os.environ['HF_HOME'])\\n",
        "os.environ.setdefault('TORCH_HOME', '/mnt/c/ai_cache/torch')\\n",
        "os.environ.setdefault('XDG_CACHE_HOME', '/mnt/c/ai_cache')\\n",
        "os.environ.setdefault('PIP_CACHE_DIR', '/mnt/c/ai_cache/pip')\\n",
        "\\n",
        "for key in ('HF_HOME', 'TORCH_HOME', 'XDG_CACHE_HOME', 'PIP_CACHE_DIR'):\\n",
        "    Path(os.environ[key]).expanduser().mkdir(parents=True, exist_ok=True)\\n",
        "\\n",
        "# Pipeline benchmark (text workload)\\n",
        "!PYTHONPATH=src python scripts/benchmark_pipeline.py --chars 10000 --runs 10\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ONNX benchmark (requires a local ONNX model)\\n",
        "# !PYTHONPATH=src python scripts/benchmark_onnx_ner.py --onnx-model /mnt/c/ai_models/detection/edge_deid/bert-ner-zh.onnx --tokenizer-dir /mnt/c/ai_models/detection/edge_deid/bert-ner-zh\\n",
        "print('Uncomment the command above once you have a local ONNX model file.')\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Opt-in perf regression tests\\n",
        "# !RUN_PERF_TESTS=1 PYTHONPATH=src pytest -q\\n",
        "print('Perf tests are skipped by default.')\\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
